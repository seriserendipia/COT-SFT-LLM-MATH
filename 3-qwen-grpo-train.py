# mamba activate torch-env

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig
from trl import GRPOTrainer, GRPOConfig
import torch
import time

# ============================================================================
# æ€§èƒ½åˆ†æå·¥å…·
# ============================================================================
class Timer:
    def __init__(self, name):
        self.name = name
    def __enter__(self):
        self.start = time.time()
        print(f"\nâ±ï¸  [{self.name}] å¼€å§‹...")
        return self
    def __exit__(self, *args):
        elapsed = time.time() - self.start
        print(f"âœ… [{self.name}] å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/60:.2f}åˆ†é’Ÿ)")
        return False

# æ£€æŸ¥GPUå¯ç”¨æ€§
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    gpu_name = torch.cuda.get_device_name(0)
    print(f"GPU name: {gpu_name}")
    print(f"Current device: {torch.cuda.current_device()}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸å…¼å®¹çš„ P100
    if "P100" in gpu_name:
        print("\n" + "="*70)
        print("âŒ ERROR: P100 GPU (CUDA 6.0) is not compatible with PyTorch 2.8+")
        print("   Minimum requirement: CUDA capability 7.0 (V100 or newer)")
        print("   Please resubmit the job to get a different GPU.")
        print("="*70)
        import sys
        sys.exit(1)
else:
    print("WARNING: CUDA not available, will use CPU (very slow!)")

# 1. é…ç½® 4-bit é‡åŒ–å‚æ•°
with Timer("é‡åŒ–é…ç½®"):
    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # ä½¿ç”¨ NF4 é‡åŒ–
    bnb_4bit_compute_dtype=torch.float16,  # ä½¿ç”¨ float16ï¼ˆä¸ fp16 è®­ç»ƒä¸€è‡´ï¼‰
        bnb_4bit_use_double_quant=True,
    )

# 2. å‡†å¤‡æ•°æ® - ä½¿ç”¨ step_list ä½œä¸º CoT æ¨ç†è¿‡ç¨‹
with Timer("æ•°æ®é›†åŠ è½½"):
    ds = load_dataset("Kanan275/GSM8k-CoT", "default", split="train[:923]")

def to_chat(e):
    """
    å°†æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸º GRPO è®­ç»ƒæ ¼å¼
    è¾“å…¥å­—æ®µï¼š
    - instruction: é—®é¢˜æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰
    - step_list: CoTæ¨ç†æ­¥éª¤ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    - final_answer: æœ€ç»ˆç­”æ¡ˆï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    
    è¾“å‡ºæ ¼å¼ï¼š
    - prompt: é—®é¢˜æ–‡æœ¬ï¼ˆGRPO å¿…éœ€å­—æ®µï¼Œä¸æ˜¯ queryï¼ï¼‰
    - ground_truth: æ­£ç¡®ç­”æ¡ˆï¼ˆç”¨äºå¥–åŠ±å‡½æ•°è¯„ä¼°ï¼‰
    """
    import json
    import ast
    
    # å¤„ç† step_list - å¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€JSONå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    steps = e["step_list"]
    if isinstance(steps, str):
        try:
            steps = json.loads(steps)
        except json.JSONDecodeError:
            try:
                steps = ast.literal_eval(steps)
            except (ValueError, SyntaxError):
                steps = [steps]
    
    if isinstance(steps, list):
        think_process = "\n".join(str(s) for s in steps)
    else:
        think_process = str(steps)
    
    # å¤„ç† final_answer - å¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€JSONå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    final_ans = e["final_answer"]
    if isinstance(final_ans, str):
        try:
            final_ans = json.loads(final_ans)
            if isinstance(final_ans, list) and len(final_ans) > 0:
                final_ans = final_ans[0]
        except json.JSONDecodeError:
            try:
                final_ans = ast.literal_eval(final_ans)
                if isinstance(final_ans, list) and len(final_ans) > 0:
                    final_ans = final_ans[0]
            except (ValueError, SyntaxError):
                pass
    elif isinstance(final_ans, list) and len(final_ans) > 0:
        final_ans = final_ans[0]
    
    final_ans = str(final_ans)
    
    # GRPO éœ€è¦ 'prompt' å­—æ®µï¼ˆä¸æ˜¯ queryï¼ï¼‰
    # åŒæ—¶ä¿å­˜ ground_truth ç”¨äºå¥–åŠ±å‡½æ•°è¯„ä¼°
    return {
        "prompt": e['instruction'],  # GRPO è®­ç»ƒå™¨æœŸæœ›çš„å­—æ®µå
        "ground_truth": final_ans.strip()  # ç”¨äºå¥–åŠ±å‡½æ•°éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§
    }

with Timer("æ•°æ®é¢„å¤„ç†"):
    ds = ds.map(to_chat)

# 3. åŠ è½½ Qwen-2.5-1.5B æ¨¡å‹å’Œåˆ†è¯å™¨
model_id = "Qwen/Qwen2.5-Coder-1.5B"

with Timer("æ¨¡å‹åŠ è½½"):
    model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
        torch_dtype=torch.float16,  # ä¸é‡åŒ–å’Œè®­ç»ƒç²¾åº¦ä¿æŒä¸€è‡´
    )
    tok = AutoTokenizer.from_pretrained(model_id)
    # ç¡®ä¿ Qwen çš„ pad_token è®¾ç½®æ­£ç¡®
    tok.pad_token = tok.eos_token
    model.config.use_cache = False

# 4. é…ç½® LoRA å‚æ•°ï¼ˆTRL æœ€ä½³å®è·µï¼‰
lora_config = LoraConfig(
    r=16, # Rå€¼
    lora_alpha=32, # Alphaå€¼ï¼ˆé€šå¸¸æ˜¯rçš„2å€ï¼‰
    # âœ… TRLæ¨èï¼štarget_modules="all-linear" æ€§èƒ½æ›´å¥½
    # ä½†å¯¹äºå°æ¨¡å‹å’Œæµ‹è¯•ï¼ŒåŒ…å«ä¸»è¦çº¿æ€§å±‚å³å¯
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"],  # åŒ…å«MLPå±‚
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# 5. å®šä¹‰ GRPO è®­ç»ƒå‚æ•°ï¼ˆåŸºäº TRL æœ€ä½³å®è·µä¼˜åŒ–ï¼‰
training_args = GRPOConfig(
    output_dir="qwen_grpo_lora",
    num_train_epochs=1,
    
    # ============================================================================
    # æ‰¹æ¬¡é…ç½®ï¼ˆåˆ©ç”¨ A100 80GB çš„å……è¶³æ˜¾å­˜ï¼‰
    # ============================================================================
    per_device_train_batch_size=2,  # ä» 1 å¢åŠ åˆ° 2ï¼ˆTRL æ¨èå€¼ï¼‰
    gradient_accumulation_steps=4,   # ä¿æŒä¸å˜ï¼Œæœ‰æ•ˆæ‰¹æ¬¡ = 2*4 = 8
    
    # ============================================================================
    # ç”Ÿæˆé…ç½®ï¼ˆGRPOæ ¸å¿ƒå‚æ•°ï¼‰
    # ============================================================================
    num_generations=4,  # æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªå€™é€‰å›ç­”
    generation_batch_size=4,  # âœ… TRLæ¨èï¼šæ˜ç¡®è®¾ç½®ç”Ÿæˆæ‰¹æ¬¡å¤§å°
    max_completion_length=512,  # é€‚åˆæ•°å­¦ CoT æ¨ç†
    max_prompt_length=1024,
    
    # ============================================================================
    # é‡‡æ ·å‚æ•°ï¼ˆå¢åŠ ç”Ÿæˆå¤šæ ·æ€§ï¼‰
    # ============================================================================
    temperature=0.9,  # TRL æ¨èå€¼ï¼Œå¢åŠ æ¢ç´¢
    top_p=0.9,        # Nucleus é‡‡æ ·
    top_k=50,         # âœ… æ·»åŠ Top-Ké‡‡æ ·
    
    # ============================================================================
    # ä¼˜åŒ–å™¨é…ç½®ï¼ˆRL è®­ç»ƒéœ€è¦è¾ƒå°å­¦ä¹ ç‡ï¼‰
    # ============================================================================
    learning_rate=5e-5,  # ä» 2e-4 é™ä½åˆ° 5e-5ï¼ˆå‚è€ƒ TRL LoRA ç¤ºä¾‹ï¼‰
    warmup_ratio=0.1,    # æ·»åŠ  warmupï¼ˆ10% æ­¥æ•°ï¼‰ï¼Œé˜²æ­¢æ—©æœŸè®­ç»ƒä¸ç¨³å®š
    max_grad_norm=1.0,   # âœ… TRLæ¨èï¼šæ¢¯åº¦è£å‰ªé˜²æ­¢è®­ç»ƒä¸ç¨³å®š
    optim="paged_adamw_8bit",
    
    # ============================================================================
    # GRPOç‰¹å®šå‚æ•°
    # ============================================================================
    beta=0.0,  # KLæ•£åº¦ç³»æ•°ï¼ˆ0=ä¸ä½¿ç”¨KLæƒ©ç½šï¼Œå‚è€ƒDAPOè®ºæ–‡ï¼‰
    
    # ============================================================================
    # æ˜¾å­˜ä¼˜åŒ–ï¼ˆ4-bité‡åŒ– + å•GPUï¼‰
    # ============================================================================
    gradient_checkpointing=True,  # âœ… TRLå¼ºçƒˆæ¨èï¼šèŠ‚çœæ˜¾å­˜
    fp16=True,  # ä½¿ç”¨ FP16ï¼ˆä¸é‡åŒ–ä¸€è‡´ï¼‰
    
    # ============================================================================
    # æ•°æ®åŠ è½½åŠ é€Ÿï¼ˆå¤šè¿›ç¨‹é¢„å¤„ç†ï¼‰
    # ============================================================================
    dataloader_num_workers=4,  # åˆ©ç”¨8ä¸ªCPUä¸­çš„4ä¸ªï¼ŒåŠ é€Ÿæ•°æ®åŠ è½½
    dataloader_pin_memory=True,  # åŠ é€Ÿ CPU->GPU æ•°æ®ä¼ è¾“
    
    # ============================================================================
    # æ—¥å¿—å’Œä¿å­˜ç­–ç•¥
    # ============================================================================
    logging_steps=5,     # æ›´é¢‘ç¹çš„æ—¥å¿—ï¼ˆä» 10 æ”¹ä¸º 5ï¼‰
    save_strategy="steps",    # æŒ‰æ­¥æ•°ä¿å­˜è€Œé epoch
    save_steps=50,            # æ¯ 50 æ­¥ä¿å­˜ä¸€æ¬¡
    save_total_limit=3,       # åªä¿ç•™æœ€è¿‘ 3 ä¸ªæ£€æŸ¥ç‚¹
)

# 6. å®šä¹‰å¥–åŠ±å‡½æ•°ï¼ˆGRPO éœ€è¦ï¼‰
def reward_func(completions, ground_truth=None, **kwargs):
    """
    GRPO å¥–åŠ±å‡½æ•°ï¼šä»…è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§
    
    è®¾è®¡åŸåˆ™ï¼š
    - æ¯ä¸ªè¾“å‡ºå¯¹åº”ä¸€ä¸ªæ ‡é‡å¥–åŠ±ï¼ˆoutcome-based rewardï¼‰
    - åªè¯„ä¼°æœ€ç»ˆç»“æœï¼Œä¸å¯¹æ¨ç†è¿‡ç¨‹ã€æ ¼å¼ã€é•¿åº¦ç­‰ä¸­é—´ç‰¹å¾æ‰“åˆ†
    - GRPO æ¡†æ¶ä¼šè‡ªåŠ¨å½’ä¸€åŒ–: rÌƒáµ¢ = (ráµ¢ - mean(r)) / std(r)
    - å½’ä¸€åŒ–åçš„å¥–åŠ±ä¼šåˆ†é…ç»™è¯¥è¾“å‡ºçš„æ‰€æœ‰ token
    
    å‚æ•°ï¼š
    - completions: ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    - ground_truth: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨ï¼ˆå¿…éœ€ï¼Œç”¨äºè¯„ä¼°å‡†ç¡®æ€§ï¼‰
    - **kwargs: å…¶ä»–å‚æ•°ï¼ˆprompts, completions_ids ç­‰ï¼‰
    
    è¿”å›: 
    - List[float]: æ¯ä¸ª completion ä¸€ä¸ªæ ‡é‡åˆ†æ•°
        * 1.0: ç­”æ¡ˆå®Œå…¨æ­£ç¡®
        * 0.0: ç­”æ¡ˆé”™è¯¯
    
    å¥–åŠ±æœºåˆ¶è¯´æ˜ï¼š
    é€šè¿‡å¤šæ¬¡é‡‡æ · (num_generations=4)ï¼ŒGRPO ä¼šå¯¹æ¯”ä¸åŒå€™é€‰çš„å¥–åŠ±ï¼š
    - é«˜å¥–åŠ±æ ·æœ¬ï¼ˆç­”æ¡ˆæ­£ç¡®ï¼‰â†’ å¢åŠ å…¶ç”Ÿæˆæ¦‚ç‡
    - ä½å¥–åŠ±æ ·æœ¬ï¼ˆç­”æ¡ˆé”™è¯¯ï¼‰â†’ é™ä½å…¶ç”Ÿæˆæ¦‚ç‡
    æ¨¡å‹ä¼šè‡ªç„¶å­¦åˆ°"å“ªäº›æ¨ç†è·¯å¾„å¯¼è‡´æ­£ç¡®ç­”æ¡ˆ"ï¼Œæ— éœ€äººå·¥è®¾è®¡è¿‡ç¨‹å¥–åŠ±
    """
    import re
    
    rewards = []
    
    for i, completion in enumerate(completions):
        # é»˜è®¤å¥–åŠ±ä¸º 0ï¼ˆé”™è¯¯ï¼‰
        reward = 0.0
        
        # å¿…é¡»æœ‰ ground_truth æ‰èƒ½è¯„ä¼°
        if ground_truth is None or i >= len(ground_truth):
            # å¦‚æœæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆï¼Œæ— æ³•åˆ¤æ–­å¯¹é”™ï¼Œç»™äºˆä¸­æ€§å¥–åŠ±
            rewards.append(0.5)
            continue
        
        # 1. æå–æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ
        # ç­–ç•¥ï¼šä¼˜å…ˆæå– </think> æ ‡ç­¾åçš„å†…å®¹ï¼Œå¦åˆ™ä½¿ç”¨å…¨æ–‡
        predicted_text = completion
        if "</think>" in completion:
            # å¦‚æœæœ‰ CoT æ ‡ç­¾ï¼Œæå–æ ‡ç­¾åçš„ç­”æ¡ˆéƒ¨åˆ†
            predicted_text = completion.split("</think>")[-1].strip()
        
        # 2. æå–ç­”æ¡ˆä¸­çš„æ•°å­—ï¼ˆGSM8K æ•°å­¦é¢˜çš„ç­”æ¡ˆé€šå¸¸æ˜¯æ•°å­—ï¼‰
        # ä½¿ç”¨æ­£åˆ™æå–æ‰€æœ‰æ•°å­—ï¼ˆåŒ…æ‹¬è´Ÿæ•°ã€å°æ•°ï¼‰
        predicted_numbers = re.findall(r'-?\d+\.?\d*', predicted_text)
        gt = str(ground_truth[i]).strip()
        gt_numbers = re.findall(r'-?\d+\.?\d*', gt)
        
        # 3. äºŒå…ƒåˆ¤æ–­ï¼šç­”æ¡ˆæ­£ç¡® vs é”™è¯¯
        if predicted_numbers and gt_numbers:
            # æ¯”è¾ƒæœ€åä¸€ä¸ªæ•°å­—ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
            if predicted_numbers[-1] == gt_numbers[-1]:
                reward = 1.0  # âœ… ç­”æ¡ˆæ­£ç¡®
            else:
                reward = 0.0  # âŒ ç­”æ¡ˆé”™è¯¯
        else:
            # å¦‚æœæ— æ³•æå–æ•°å­—ï¼Œå°è¯•ç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
            if gt.lower() in predicted_text.lower():
                reward = 1.0  # âœ… æ–‡æœ¬åŒ¹é…æˆåŠŸ
            else:
                reward = 0.0  # âŒ æ— æ³•åŒ¹é…
        
        rewards.append(float(reward))
    
    # è¿”å›åŸå§‹å¥–åŠ±ï¼ŒGRPO æ¡†æ¶ä¼šè‡ªåŠ¨æ‰§è¡Œå½’ä¸€åŒ–ï¼š
    # rÌƒáµ¢ = (ráµ¢ - mean(r)) / std(r)
    # ç„¶åå°† rÌƒáµ¢ åˆ†é…ç»™è¾“å‡º oáµ¢ çš„æ‰€æœ‰ token ä½œä¸ºä¼˜åŠ¿å‡½æ•°
    return rewards

# 7. åˆå§‹åŒ–å¹¶å¼€å§‹è®­ç»ƒ
with Timer("è®­ç»ƒå™¨åˆå§‹åŒ–"):
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=ds,
        peft_config=lora_config,
        processing_class=tok, # TRL 0.23.0 æ¨èä½¿ç”¨ processing_class
        reward_funcs=reward_func, # æ³¨æ„ï¼šå‚æ•°åæ˜¯ reward_funcsï¼ˆå¤æ•°ï¼‰ï¼Œä¸æ˜¯ reward_fnï¼
    )

print("\n" + "="*80)
print("ğŸš€ å¼€å§‹ GRPO è®­ç»ƒ...")
print(f"ğŸ“Š é…ç½®: {len(ds)} æ ·æœ¬ Ã— {training_args.num_generations} ç”Ÿæˆ = {len(ds) * training_args.num_generations} æ¬¡æ¨ç†")
print(f"ğŸ’¾ æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size} Ã— {training_args.gradient_accumulation_steps} = {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps} (æœ‰æ•ˆ)")
print("="*80 + "\n")

with Timer("GRPO å®Œæ•´è®­ç»ƒ"):
    trainer.train()
