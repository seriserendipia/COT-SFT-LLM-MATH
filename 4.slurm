#!/bin/bash
#SBATCH --account=liu32_1378
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --ntasks=1

# ============================================================================
# GRPO 模型推理评估资源配置
# ============================================================================
# 数据集大小 | 样本数 | 时间       | 内存  | CPU | GPU | 说明
# -----------|--------|-----------|-------|-----|-----|------------------
# 快速测试   | 10     | 00:15:00  | 16G   | 4   | 1   | 验证模型加载和推理
# 完整评估   | 395    | 00:30:00  | 24G   | 8   | 1   | 完整测试集评估
# ============================================================================

# ============================================================================
# GPU 策略：不指定型号 = 接受所有可用 GPU（L40S/A100/A40/V100）
# 优点：大大提高获得资源速度，减少排队时间
# 注意：系统可能分配 P100（CUDA 6.0），代码会自动检测并报警
# ============================================================================
# 可用 GPU 型号（按性能排序）：
#   l40s (CUDA 8.9) > a100 (CUDA 8.0) > a40 (CUDA 8.6) > v100 (CUDA 7.0)
#   ⚠️ p100 (CUDA 6.0) 不兼容 PyTorch 2.8+
# 
# 推理任务特点：
#   - 相比训练，推理对GPU性能要求更低
#   - V100/A40 已经足够快
#   - 建议不指定GPU型号，加快排队速度
# ============================================================================

# ============================================================================
# 方案A：快速测试配置（10条样本）- 当前默认
# 用途：验证 GRPO 模型加载、推理流程、结果保存
# 数据：train[924:934]（代码默认）
# ============================================================================
# #SBATCH --cpus-per-task=4
# #SBATCH --mem=16G
# #SBATCH --time=00:15:00
# #SBATCH --gpus-per-task=1

# ============================================================================
# 方案B：完整评估配置（395条样本）
# 用途：完整测试集评估，获得准确的 accuracy 指标
# 数据：train[924:1319]（需要修改代码第45行）
# 使用方法：注释掉上方方案A的4行，取消下方4行的注释
# ============================================================================
#SBATCH --cpus-per-task=8
#SBATCH --mem=24G
#SBATCH --time=12:30:00
#SBATCH --gpus-per-task=1

# ============================================================================
# 邮件通知配置
# ============================================================================
#SBATCH --mail-user=yihelu@usc.edu
#SBATCH --mail-type=END,FAIL

# ============================================================================
# 如果排队太久，可取消以下注释指定特定GPU型号：
# #SBATCH --gpus-per-task=v100:1    # 推荐：性能够用，资源充足
# #SBATCH --gpus-per-task=a40:1     # 备选：性能强，资源相对充足
# SBATCH --gpus-per-task=a100:1    # 高性能但排队长
# #SBATCH --gpus-per-task=l40s:1    # 最强但排队最长
# ============================================================================

echo ""
echo "=========================================="
echo "🚀 GRPO Model Inference Job Starting"
echo "=========================================="
echo "Job started at: `date`"
echo "Running on hosts: $SLURM_NODELIST"
echo "Running on $SLURM_NNODES nodes"
echo "Running on $SLURM_NPROCS processors"
echo "GPU allocation: $CUDA_VISIBLE_DEVICES"
echo "=========================================="
echo ""

# Move to the correct directory
cd /home1/yihelu/csci566/
echo "Current working directory: `pwd`"

# Load conda environment
source /apps/conda/miniforge3/24.3.0/etc/profile.d/conda.sh
conda activate torch-env

# ============================================================================
# 运行 GRPO 模型推理评估
# ============================================================================
# 
# 脚本功能：
#   1. 加载 GRPO 训练的 LoRA 模型（qwen_grpo_lora_multigpu）
#   2. 在测试集上进行推理（train[924:934] 或 train[924:1319]）
#   3. 计算 accuracy 并保存详细结果
#   4. 输出目录：inference_results_grpo/
# 
# 输出文件：
#   - inference_results.json      : 所有样本的推理结果
#   - evaluation_metrics.json     : accuracy 等评估指标
#   - wrong_cases.json            : 错误案例分析
# 
# 配置切换：
#   - 当前：10条测试（train[924:934]）
#   - 完整评估：修改代码第45行为 train[924:1319]
#   - 同时切换 SLURM 配置到方案B（更多资源）
# 
echo "📊 Starting GRPO model inference..."
python 4-qwen-grpo-inference.py
# 
# ============================================================================

# Check exit status
EXIT_CODE=$?
echo ""
echo "=========================================="
if [ $EXIT_CODE -eq 0 ]; then
    echo "✅ Inference completed successfully!"
    echo "📁 Results saved in: inference_results_grpo/"
else
    echo "❌ Inference failed with exit code: $EXIT_CODE"
fi
echo "Job finished at: `date`"
echo "=========================================="
echo ""

exit $EXIT_CODE
