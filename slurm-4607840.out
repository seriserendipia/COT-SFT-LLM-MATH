==========================================
SLURM_JOB_ID = 4607840
SLURM_JOB_NODELIST = b18-16
TMPDIR = /tmp/SLURM_4607840
==========================================

Starting at Thu Oct 30 13:49:33 PDT 2025
Running on hosts: b18-16
Running on 1 nodes.
Running on 1 processors.
GPU allocation: 0,1
Current working directory is /home1/yihelu/csci566
PyTorch version: 2.8.0+cu128
CUDA available: True
CUDA version: 12.8
GPU count: 2
  GPU 0: NVIDIA L40S
  GPU 1: NVIDIA L40S

â±ï¸  [é‡åŒ–é…ç½®] å¼€å§‹...
PyTorch version: 2.8.0+cu128
CUDA available: True
CUDA version: 12.8
GPU count: 2
  GPU 0: NVIDIA L40S
  GPU 1: NVIDIA L40S

â±ï¸  [é‡åŒ–é…ç½®] å¼€å§‹...
âœ… [é‡åŒ–é…ç½®] å®Œæˆï¼Œè€—æ—¶: 0.00ç§’ (0.00åˆ†é’Ÿ)

â±ï¸  [æ•°æ®é›†åŠ è½½] å¼€å§‹...
âœ… [é‡åŒ–é…ç½®] å®Œæˆï¼Œè€—æ—¶: 0.00ç§’ (0.00åˆ†é’Ÿ)

â±ï¸  [æ•°æ®é›†åŠ è½½] å¼€å§‹...
âœ… [æ•°æ®é›†åŠ è½½] å®Œæˆï¼Œè€—æ—¶: 1.39ç§’ (0.02åˆ†é’Ÿ)

â±ï¸  [æ•°æ®é¢„å¤„ç†] å¼€å§‹...
âœ… [æ•°æ®é¢„å¤„ç†] å®Œæˆï¼Œè€—æ—¶: 0.00ç§’ (0.00åˆ†é’Ÿ)

â±ï¸  [æ¨¡å‹åŠ è½½] å¼€å§‹...
`torch_dtype` is deprecated! Use `dtype` instead!
âœ… [æ•°æ®é›†åŠ è½½] å®Œæˆï¼Œè€—æ—¶: 1.61ç§’ (0.03åˆ†é’Ÿ)

â±ï¸  [æ•°æ®é¢„å¤„ç†] å¼€å§‹...
âœ… [æ•°æ®é¢„å¤„ç†] å®Œæˆï¼Œè€—æ—¶: 0.00ç§’ (0.00åˆ†é’Ÿ)

â±ï¸  [æ¨¡å‹åŠ è½½] å¼€å§‹...
`torch_dtype` is deprecated! Use `dtype` instead!
âœ… [æ¨¡å‹åŠ è½½] å®Œæˆï¼Œè€—æ—¶: 4.66ç§’ (0.08åˆ†é’Ÿ)

â±ï¸  [è®­ç»ƒå™¨åˆå§‹åŒ–] å¼€å§‹...
âœ… [æ¨¡å‹åŠ è½½] å®Œæˆï¼Œè€—æ—¶: 4.55ç§’ (0.08åˆ†é’Ÿ)

â±ï¸  [è®­ç»ƒå™¨åˆå§‹åŒ–] å¼€å§‹...
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
âœ… [è®­ç»ƒå™¨åˆå§‹åŒ–] å®Œæˆï¼Œè€—æ—¶: 0.24ç§’ (0.00åˆ†é’Ÿ)

================================================================================
ğŸš€ å¼€å§‹å¤šGPU GRPO è®­ç»ƒ...
ğŸ® GPUæ•°é‡: 2
ğŸ“Š é…ç½®: 10 æ ·æœ¬ Ã— 4 ç”Ÿæˆ = 40 æ¬¡æ¨ç†
ğŸ’¾ æ€»æ‰¹æ¬¡å¤§å°: 2 Ã— 2 Ã— 2 = 8
================================================================================


â±ï¸  [GRPO å®Œæ•´è®­ç»ƒ] å¼€å§‹...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
âœ… [è®­ç»ƒå™¨åˆå§‹åŒ–] å®Œæˆï¼Œè€—æ—¶: 0.20ç§’ (0.00åˆ†é’Ÿ)

================================================================================
ğŸš€ å¼€å§‹å¤šGPU GRPO è®­ç»ƒ...
ğŸ® GPUæ•°é‡: 2
ğŸ“Š é…ç½®: 10 æ ·æœ¬ Ã— 4 ç”Ÿæˆ = 40 æ¬¡æ¨ç†
ğŸ’¾ æ€»æ‰¹æ¬¡å¤§å°: 2 Ã— 2 Ã— 2 = 8
================================================================================


â±ï¸  [GRPO å®Œæ•´è®­ç»ƒ] å¼€å§‹...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
âœ… [GRPO å®Œæ•´è®­ç»ƒ] å®Œæˆï¼Œè€—æ—¶: 0.21ç§’ (0.00åˆ†é’Ÿ)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home1/yihelu/csci566/3-5-qwen-grpo-train-multigpu.py", line 319, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/transformers/trainer.py", line 2328, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/transformers/trainer.py", line 2483, in _inner_training_loop
[rank1]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank1]:                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/accelerator.py", line 1555, in prepare
[rank1]:     result = tuple(
[rank1]:              ^^^^^^
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/accelerator.py", line 1556, in <genexpr>
[rank1]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank1]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/accelerator.py", line 1398, in _prepare_one
[rank1]:     return self.prepare_model(obj, device_placement=device_placement)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/accelerator.py", line 1786, in prepare_model
[rank1]:     raise ValueError(
[rank1]: ValueError: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`
W1030 13:49:50.503000 2537357 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2537398 closing signal SIGTERM
E1030 13:49:50.730000 2537357 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 2537399) of binary: /home1/yihelu/.conda/envs/torch-env/bin/python3.12
Traceback (most recent call last):
  File "/home1/yihelu/.conda/envs/torch-env/bin/accelerate", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/yihelu/.conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
3-5-qwen-grpo-train-multigpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-30_13:49:50
  host      : b18-16.hpc.usc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2537399)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

Program finished with exit code 0 at: Thu Oct 30 13:49:51 PDT 2025

