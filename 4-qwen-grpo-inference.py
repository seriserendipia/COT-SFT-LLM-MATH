# mamba activate torch-env
# GRPO æ¨¡å‹æ¨ç†å’Œè¯„ä¼°è„šæœ¬

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch
import json
import re
from tqdm import tqdm
import os

# æ£€æŸ¥GPUå¯ç”¨æ€§
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    gpu_name = torch.cuda.get_device_name(0)
    print(f"GPU name: {gpu_name}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸å…¼å®¹çš„ P100
    if "P100" in gpu_name:
        print("\n" + "="*70)
        print("âŒ ERROR: P100 GPU (CUDA 6.0) is not compatible with PyTorch 2.8+")
        print("   Minimum requirement: CUDA capability 7.0 (V100 or newer)")
        print("   Please resubmit the job to get a different GPU.")
        print("="*70)
        import sys
        sys.exit(1)
else:
    print("WARNING: CUDA not available, will use CPU (very slow!)")

# 1. é…ç½® 4-bit é‡åŒ–å‚æ•°
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# 2. åŠ è½½æ•°æ®é›†ï¼ˆæµ‹è¯•é›†ï¼štrain[924:1319]ï¼‰
print("Loading dataset...")
# å…ˆç”¨å‰10æ¡æµ‹è¯•
# ds = load_dataset("Kanan275/GSM8k-CoT", "default", split="train[924:934]")
# å®Œæ•´æµ‹è¯•é›†ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢ä¸€è¡Œï¼‰ï¼š
ds = load_dataset("Kanan275/GSM8k-CoT", "default", split="train[924:1319]")

def to_chat(e):
    """
    å°†æ•°æ®é›†æ ¼å¼è½¬æ¢ä¸ºæ¨ç†æ ¼å¼
    è¾“å…¥å­—æ®µï¼š
    - instruction: é—®é¢˜æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰
    - step_list: CoTæ¨ç†æ­¥éª¤ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    - final_answer: æœ€ç»ˆç­”æ¡ˆï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
    
    è¾“å‡ºæ ¼å¼ï¼š
    - question: é—®é¢˜æ–‡æœ¬
    - answer: <think>æ¨ç†æ­¥éª¤</think>\næœ€ç»ˆç­”æ¡ˆ
    - ground_truth: æœ€ç»ˆç­”æ¡ˆï¼ˆç”¨äºè¯„ä¼°ï¼‰
    """
    import json
    import ast
    
    # å¤„ç† step_list - å¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€JSONå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    steps = e["step_list"]
    if isinstance(steps, str):
        try:
            steps = json.loads(steps)
        except json.JSONDecodeError:
            try:
                steps = ast.literal_eval(steps)
            except (ValueError, SyntaxError):
                steps = [steps]
    
    if isinstance(steps, list):
        think_process = "\n".join(str(s) for s in steps)
    else:
        think_process = str(steps)
    
    # å¤„ç† final_answer - å¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€JSONå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
    final_ans = e["final_answer"]
    if isinstance(final_ans, str):
        try:
            final_ans = json.loads(final_ans)
            if isinstance(final_ans, list) and len(final_ans) > 0:
                final_ans = final_ans[0]
        except json.JSONDecodeError:
            try:
                final_ans = ast.literal_eval(final_ans)
                if isinstance(final_ans, list) and len(final_ans) > 0:
                    final_ans = final_ans[0]
            except (ValueError, SyntaxError):
                pass
    elif isinstance(final_ans, list) and len(final_ans) > 0:
        final_ans = final_ans[0]
    
    final_ans = str(final_ans)
    
    # æ„å»ºå®Œæ•´ç­”æ¡ˆ
    full_answer = f"<think>{think_process.strip()}</think>\n{final_ans.strip()}"
    ground_truth = final_ans.strip()  # ç”¨äºè¯„ä¼°çš„æ ‡å‡†ç­”æ¡ˆ
    
    return {
        "question": e['instruction'], 
        "answer": full_answer,
        "ground_truth": ground_truth
    }

ds = ds.map(to_chat)
print(f"Loaded {len(ds)} test samples")

# 3. åŠ è½½åŸºç¡€æ¨¡å‹
print("Loading base model...")
model_id = "Qwen/Qwen2.5-Coder-1.5B"
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
)

# 4. åŠ è½½GRPOè®­ç»ƒçš„LoRAæƒé‡
print("Loading GRPO LoRA adapter...")

# è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
import glob
base_lora_dir = "qwen_grpo_lora_multigpu"

# æŸ¥æ‰¾æ‰€æœ‰ checkpoint-* ç›®å½•
checkpoint_dirs = glob.glob(os.path.join(base_lora_dir, "checkpoint-*"))

if checkpoint_dirs:
    # æŒ‰ checkpoint ç¼–å·æ’åºï¼Œé€‰æ‹©æœ€å¤§çš„ï¼ˆæœ€æ–°çš„ï¼‰
    checkpoint_dirs.sort(key=lambda x: int(x.split("-")[-1]))
    lora_path = checkpoint_dirs[-1]  # æœ€åä¸€ä¸ª = ç¼–å·æœ€å¤§ = æœ€æ–°
    print(f"Found {len(checkpoint_dirs)} checkpoints, using latest: {lora_path}")
else:
    # å¦‚æœæ²¡æœ‰ checkpoint å­ç›®å½•ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨æ ¹ç›®å½•
    lora_path = base_lora_dir
    print(f"No checkpoints found, using root directory: {lora_path}")

# éªŒè¯è·¯å¾„å­˜åœ¨
if not os.path.exists(lora_path):
    print(f"\n{'='*70}")
    print(f"âŒ ERROR: LoRA adapter directory not found!")
    print(f"   Looking for: {os.path.abspath(lora_path)}")
    print(f"   Current working directory: {os.getcwd()}")
    print(f"\n   Possible solutions:")
    print(f"   1. Run this script from the same directory where training was done")
    print(f"   2. Check if GRPO training completed successfully")
    print(f"   3. Use absolute path if needed")
    print("="*70)
    import sys
    sys.exit(1)

# æ£€æŸ¥å…³é”®æ–‡ä»¶
adapter_config = os.path.join(lora_path, "adapter_config.json")
if not os.path.exists(adapter_config):
    print(f"\n{'='*70}")
    print(f"âŒ ERROR: adapter_config.json not found in {lora_path}")
    print(f"   This file is required to load the LoRA adapter.")
    print(f"   GRPO training may not have completed successfully.")
    print("="*70)
    import sys
    sys.exit(1)

print(f"âœ… Found GRPO LoRA adapter at: {os.path.abspath(lora_path)}")

model = PeftModel.from_pretrained(base_model, lora_path)
model.eval()

# 5. åŠ è½½åˆ†è¯å™¨
tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tok.pad_token = tok.eos_token

# 6. æ¨ç†å‡½æ•°
def extract_answer(text):
    """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
    # å°è¯•æå– </think> ä¹‹åçš„å†…å®¹
    if "</think>" in text:
        answer = text.split("</think>")[-1].strip()
    else:
        answer = text.strip()
    
    # æå–æ•°å­—
    numbers = re.findall(r'-?\d+\.?\d*', answer)
    if numbers:
        return numbers[-1]  # è¿”å›æœ€åä¸€ä¸ªæ•°å­—
    return answer

def generate_answer(question, max_length=2048):
    """
    ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ç¡®å®šæ€§è§£ç ï¼‰
    
    ç”Ÿæˆå‚æ•°è¯´æ˜ï¼š
    - temperature=0.0: ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼‰
    - do_sample=False: å…³é—­éšæœºé‡‡æ ·
    - è¿™æ ·å¯ä»¥ä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼Œä¾¿äºè¯„ä¼°å’Œå¯¹æ¯”
    """
    prompt = f"ç”¨æˆ·: {question}\nåŠ©æ‰‹:"
    
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=max_length)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.0,           # âœ… ç¡®å®šæ€§ç”Ÿæˆï¼ˆè´ªå¿ƒè§£ç ï¼‰
            do_sample=False,           # âœ… å…³é—­é‡‡æ ·
            pad_token_id=tok.eos_token_id,
        )
    
    generated_text = tok.decode(outputs[0], skip_special_tokens=True)
    # ç§»é™¤è¾“å…¥éƒ¨åˆ†
    response = generated_text[len(prompt):].strip()
    return response

# 7. æ‰¹é‡æ¨ç†
print("\n" + "="*70)
print("ğŸš€ Starting GRPO model inference...")
print(f"ğŸ“Š Test samples: {len(ds)}")
print(f"ğŸ¯ Evaluation metric: Accuracy")
print(f"ğŸ”§ Generation mode: Deterministic (temperature=0.0)")
print("="*70 + "\n")

results = []
correct = 0
total = 0

for i, example in enumerate(tqdm(ds, desc="Evaluating")):
    question = example["question"]
    ground_truth = example["ground_truth"]
    
    # ç”Ÿæˆç­”æ¡ˆ
    generated_response = generate_answer(question)
    predicted_answer = extract_answer(generated_response)
    gt_answer = extract_answer(ground_truth)
    
    # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
    is_correct = predicted_answer == gt_answer
    if is_correct:
        correct += 1
    total += 1
    
    # ä¿å­˜ç»“æœ
    result = {
        "index": i,
        "question": question,
        "ground_truth": ground_truth,
        "ground_truth_extracted": gt_answer,
        "generated_response": generated_response,
        "predicted_answer": predicted_answer,
        "is_correct": is_correct
    }
    results.append(result)
    
    # æ¯5ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆæµ‹è¯•é›†è¾ƒå°ï¼‰
    if (i + 1) % 5 == 0:
        acc = correct / total
        print(f"\nProgress: {i+1}/{len(ds)}, Accuracy so far: {acc:.4f} ({acc*100:.2f}%)")

# 8. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
accuracy = correct / total
print(f"\n{'='*70}")
print(f"ğŸ“ˆ Final Evaluation Results (GRPO Model):")
print(f"{'='*70}")
print(f"Total samples: {total}")
print(f"Correct predictions: {correct}")
print(f"Wrong predictions: {total - correct}")
print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"{'='*70}\n")

# 9. ä¿å­˜ç»“æœ
output_dir = "inference_results_grpo"  # GRPO æ¨¡å‹çš„æ¨ç†ç»“æœ
os.makedirs(output_dir, exist_ok=True)

# ä¿å­˜è¯¦ç»†æ¨ç†ç»“æœ
results_file = os.path.join(output_dir, "inference_results.json")
with open(results_file, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
print(f"âœ… Inference results saved to: {results_file}")

# ä¿å­˜è¯„ä¼°æŒ‡æ ‡
metrics = {
    "total_samples": total,
    "correct_predictions": correct,
    "wrong_predictions": total - correct,
    "accuracy": accuracy,
    "model_type": "GRPO",
    "model_path": lora_path,
    "base_model": model_id,
    "dataset": "Kanan275/GSM8k-CoT",
    "split": "train[924:934]",  # æ›´æ–°ä¸ºå®é™…ä½¿ç”¨çš„split
    "generation_config": {
        "max_new_tokens": 512,
        "temperature": 0.0,
        "do_sample": False,
        "method": "greedy_decoding"
    }
}

metrics_file = os.path.join(output_dir, "evaluation_metrics.json")
with open(metrics_file, "w", encoding="utf-8") as f:
    json.dump(metrics, f, ensure_ascii=False, indent=2)
print(f"âœ… Evaluation metrics saved to: {metrics_file}")

# 10. ä¿å­˜é”™è¯¯æ¡ˆä¾‹åˆ†æ
wrong_cases = [r for r in results if not r["is_correct"]]
wrong_cases_file = os.path.join(output_dir, "wrong_cases.json")
with open(wrong_cases_file, "w", encoding="utf-8") as f:
    json.dump(wrong_cases, f, ensure_ascii=False, indent=2)
print(f"âœ… Wrong cases saved to: {wrong_cases_file}")
print(f"   Total wrong cases: {len(wrong_cases)}")

# 11. æ‰“å°ä¸€äº›ç¤ºä¾‹ç»“æœ
if len(results) > 0:
    print(f"\n{'='*70}")
    print("ğŸ“ Sample Results:")
    print(f"{'='*70}")
    
    # æ‰“å°ç¬¬ä¸€ä¸ªæ­£ç¡®çš„æ¡ˆä¾‹
    correct_cases = [r for r in results if r["is_correct"]]
    if correct_cases:
        sample = correct_cases[0]
        print(f"\nâœ… Correct Example (Index {sample['index']}):")
        print(f"Question: {sample['question'][:100]}...")
        print(f"Predicted: {sample['predicted_answer']}")
        print(f"Ground Truth: {sample['ground_truth_extracted']}")
    
    # æ‰“å°ç¬¬ä¸€ä¸ªé”™è¯¯çš„æ¡ˆä¾‹
    if wrong_cases:
        sample = wrong_cases[0]
        print(f"\nâŒ Wrong Example (Index {sample['index']}):")
        print(f"Question: {sample['question'][:100]}...")
        print(f"Predicted: {sample['predicted_answer']}")
        print(f"Ground Truth: {sample['ground_truth_extracted']}")
        print(f"Full Response: {sample['generated_response'][:200]}...")

print(f"\n{'='*70}")
print("ğŸ‰ GRPO Model Inference and Evaluation Completed!")
print(f"{'='*70}")
