#!/bin/bash
#SBATCH --account=liu32_1378
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --ntasks=1

# ============================================================================
# å¤šGPUè®­ç»ƒé…ç½®ï¼ˆé€‚åˆå¤§æ•°æ®é›†ï¼‰
# ============================================================================
# æ•°æ®é›†å¤§å° | GPUæ•° | æ—¶é—´       | å†…å­˜  | CPU | è¯´æ˜
# -----------|-------|-----------|-------|-----|------------------
# 100 æ ·æœ¬   | 2     | 00:20:00  | 48G   | 16  | 2å€åŠ é€Ÿ
# 1000 æ ·æœ¬  | 2     | 01:00:00  | 64G   | 16  | ä¸­ç­‰è§„æ¨¡
# å…¨æ•°æ®é›†   | 4     | 04:00:00  | 96G   | 32  | å®Œæ•´è®­ç»ƒåŠ é€Ÿ
# ============================================================================

# å½“å‰é…ç½®ï¼š2Ã—GPU æµ‹è¯•é…ç½®
#SBATCH --cpus-per-task=16
#SBATCH --mem=64G
#SBATCH --time=12:05:00
#SBATCH --gpus-per-task=2  # ç”³è¯·2ä¸ªGPU
#SBATCH --mail-user=yihelu@usc.edu
#SBATCH --mail-type=END,FAIL


# ============================================================================
# å¤šGPUç­–ç•¥è¯´æ˜ï¼š
# 1. PyTorch DDP (DistributedDataParallel)ï¼šè‡ªåŠ¨å¯ç”¨ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
# 2. accelerate åº“ä¼šè‡ªåŠ¨æ£€æµ‹ SLURM ç¯å¢ƒå˜é‡å¹¶åˆå§‹åŒ–å¤šGPU
# 3. æ¯ä¸ªGPUè¿è¡Œç›¸åŒä»£ç ï¼Œä½†å¤„ç†ä¸åŒæ‰¹æ¬¡æ•°æ®
# 4. æ¢¯åº¦åœ¨GPUé—´è‡ªåŠ¨åŒæ­¥
# ============================================================================

echo ""
echo "Starting at `date`"
echo "Running on hosts: $SLURM_NODELIST"
echo "Running on $SLURM_NNODES nodes."
echo "Running on $SLURM_NPROCS processors."
echo "GPU allocation: $CUDA_VISIBLE_DEVICES"

# Move to the correct directory
cd /home1/yihelu/csci566/
echo "Current working directory is `pwd`"

# Load conda environment
source /apps/conda/miniforge3/24.3.0/etc/profile.d/conda.sh
conda activate torch-env

# ============================================================================
# å¤šGPU DDP è®­ç»ƒé…ç½®
# ============================================================================
# 
# âš ï¸ é‡è¦ï¼šDDP + gradient_checkpointing + LoRA å­˜åœ¨å·²çŸ¥å†²çª
# é”™è¯¯ï¼šRuntimeError: Expected to mark a variable ready only once
# è§£å†³ï¼šå¿…é¡»å…³é—­ gradient_checkpointing=Falseï¼ˆå·²åœ¨ä»£ç ä¸­è®¾ç½®ï¼‰
# 
# DDP æ¨¡å¼ç‰¹ç‚¹ï¼š
#   âœ… é…ç½®ç®€å•ï¼Œæ— éœ€é¢å¤–ä¾èµ–
#   âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆ1.8Ã— å•GPUï¼‰
#   âœ… ä¸ 4-bit é‡åŒ– + LoRA å®Œç¾å…¼å®¹
#   âŒ ä¸èƒ½ä½¿ç”¨ gradient_checkpointing
#   ğŸ“Š æ˜¾å­˜å ç”¨ï¼š~14GB/GPUï¼ˆQwen2.5-1.5Bï¼‰
# 
# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
#   - å‡å° per_device_train_batch_sizeï¼ˆä»£ç ä¸­æ”¹ä¸º 1ï¼‰
#   - å¢åŠ  gradient_accumulation_stepsï¼ˆä»£ç ä¸­æ”¹ä¸º 4ï¼‰
#   - å‡å° max_completion_lengthï¼ˆä»£ç ä¸­æ”¹ä¸º 128ï¼‰
# 
echo "ğŸš€ Launching 2-GPU DDP training..."
accelerate launch --config_file accelerate_config_2gpu.yaml 3-5-qwen-grpo-train-multigpu.py
# 
# ============================================================================

echo ""
echo "Program finished with exit code $? at: `date`"
echo ""
